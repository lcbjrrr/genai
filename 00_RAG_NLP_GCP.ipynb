{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcbjrrr/genai/blob/main/00_RAG_NLP_GCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5Phtwtbf5eA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fundamentals"
      ],
      "metadata": {
        "id": "skct5N00hxEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Natural Language Processing (NLP)"
      ],
      "metadata": {
        "id": "DHazxZdAiED1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing (NLP) is a multidisciplinary field at the intersection of computer science, artificial intelligence (AI), and linguistics. Its core goal is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful. This involves developing algorithms and models to process raw text and speech data, allowing machines to perform tasks like translation, sentiment analysis, text summarization, and voice assistants. NLP is fundamental to how people interact with technology, making systems more intuitive and capable of handling the complexity and nuance of human communication"
      ],
      "metadata": {
        "id": "CAi5srgIiIDt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eWnhxbbIid_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Neural Networks and Deep Leanring"
      ],
      "metadata": {
        "id": "4cJpMa5xifIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Deep learning, based on neural networks and specifically Recurrent Neural Networks (RNNs), revolutionized Natural Language Processing (NLP) by enabling computers to understand the sequential nature of human language. RNN variants like LSTMs and GRUs utilize an internal hidden state (memory) to maintain context across a sequence of words, overcoming the limitation of standard neural networks. This capability allows them to excel in sequential tasks such as machine translation (using encoder-decoder models), language modeling for text generation, and sentiment analysis. Although the newer Transformer architecture has become dominant, RNNs established the crucial deep learning foundation for handling the complexity, context, and dependencies inherent in text data.\n",
        "\n",
        "![](https://pbs.twimg.com/media/G55BA6oXIAA4x_y?format=jpg&name=900x900)"
      ],
      "metadata": {
        "id": "oaSIU4q8ivmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Feedforward Neural Network (FNN)"
      ],
      "metadata": {
        "id": "ijD10UByjeAM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6f57863"
      },
      "source": [
        "A forward-feeding neural network, often called a Feedforward Neural Network (FNN), is the simplest type of artificial neural network. In an FNN, information moves in only one direction—forward—from the input layer, through any hidden layers, and finally to the output layer. There are no loops or cycles, meaning data flows linearly without looping back, making them suitable for tasks like classification and regression where input is mapped directly to an output.\n",
        "\n",
        "![](https://pbs.twimg.com/media/G55BMjMWwAAFIVf?format=png&name=360x360)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recurrent Neural Networks (RNNs)"
      ],
      "metadata": {
        "id": "HYKeVK_Rkgar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Recurrent Neural Network (RNN) is a type of neural network specifically designed to process sequential data, such as text, speech, or time series. Unlike Feedforward Networks, RNNs have a loop that allows information to be passed from one step of the network to the next, effectively giving them an internal memory or hidden state. This memory enables the network to consider the context of previous elements in a sequence when processing the current one. This makes RNNs uniquely suited for tasks like natural language processing (NLP), where the meaning of a word depends heavily on the words that preceded it, allowing them to perform sequence-dependent tasks like language modeling and machine translation\n",
        "\n",
        "![](https://pbs.twimg.com/media/G55B_46WMAA12ts?format=png&name=360x360)"
      ],
      "metadata": {
        "id": "intJ8Hiik6Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "text = \"Deep learning, based on neural networks and specifically Recurrent Neural Networks (RNNs), revolutionized Natural Language Processing (NLP)\"\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_index = {char: i for i, char in enumerate(chars)}\n",
        "index_to_char = {i: char for i, char in enumerate(chars)}\n",
        "char_to_index"
      ],
      "metadata": {
        "id": "bi7Gfp9il92x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = text[0:0 + 3]\n",
        "label = text[3]\n",
        "print(seq,([char_to_index[char] for char in seq]))\n",
        "print(label,char_to_index[label])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbBlpkMGnpuX",
        "outputId": "f3b4072f-76a0-42e0-d884-1400b85a0b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dee [4, 13, 13]\n",
            "p 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 3\n",
        "sequences = []\n",
        "labels = []\n",
        "for i in range(len(text) - seq_length):\n",
        "    seq = text[i:i + seq_length]\n",
        "    label = text[i + seq_length]\n",
        "    sequences.append([char_to_index[char] for char in seq])\n",
        "    labels.append(char_to_index[label])\n",
        "\n",
        "X = np.array(sequences)\n",
        "y = np.array(labels)\n",
        "print(y)\n",
        "X"
      ],
      "metadata": {
        "id": "KcPIXBHtmdQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_one_hot = tf.one_hot(X, len(chars))\n",
        "y_one_hot = tf.one_hot(y, len(chars))\n",
        "y_one_hot"
      ],
      "metadata": {
        "id": "QRBM-ljFoaKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(50, input_shape=(seq_length, len(chars)), activation='relu'))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_one_hot, y_one_hot, epochs=100)"
      ],
      "metadata": {
        "id": "dKnwRc7sol-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_seq = \"Deep learn\"\n",
        "generated_text = start_seq\n",
        "x = np.array([[char_to_index[char] for char in generated_text[-seq_length:]]])\n",
        "print(x)\n",
        "x_one_hot = tf.one_hot(x, len(chars))\n",
        "prediction = model.predict(x_one_hot)\n",
        "print(prediction)\n",
        "next_index = np.argmax(prediction)\n",
        "print('i=',next_index)\n",
        "index_to_char[next_index]"
      ],
      "metadata": {
        "id": "GwidfmOMpDYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(60):\n",
        "    x = np.array([[char_to_index[char] for char in generated_text[-seq_length:]]])\n",
        "    x_one_hot = tf.one_hot(x, len(chars))\n",
        "    prediction = model.predict(x_one_hot)\n",
        "    next_index = np.argmax(prediction)\n",
        "    next_char = index_to_char[next_index]\n",
        "    generated_text += next_char\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "LvuB1te8qJ2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5 (Text-to-Text Transfer Transformer)"
      ],
      "metadata": {
        "id": "9bzucX34l88E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The T5 (Text-to-Text Transfer Transformer) model, developed by Google AI, revolutionized NLP by introducing a unified framework where every language task is treated as a text-to-text problem. Built on the Transformer architecture's encoder-decoder structure, T5 can handle diverse tasks—including translation, summarization, question answering, and classification—by simply feeding the input with a task-specific prefix (e.g., \"translate English to German: ...\") and receiving the output as plain text. This consistency simplifies the model design and allows a single, pre-trained model to achieve state-of-the-art results across numerous benchmarks after fine-tuning\n",
        "\n",
        "![](https://pbs.twimg.com/media/G55IpcfW8AAYrY4?format=png&name=360x360)"
      ],
      "metadata": {
        "id": "La3VVyFIrih8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers are a powerful neural network architecture that utilize an attention mechanism to weigh the importance of different parts of the input data, enabling them to process sequences in parallel and efficiently capture long-range dependencies, becoming the foundation for modern large language models like BERT and GPT\n",
        "\n",
        "![](https://pbs.twimg.com/media/G55JZMnXAAAgub2?format=jpg&name=900x900)"
      ],
      "metadata": {
        "id": "jBPBg5ZasS3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers torch sentencepiece"
      ],
      "metadata": {
        "id": "LOirAkknstt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "SwgYUN9Eripp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "T5 language model for text-to-text tasks. It imports the necessary components from the transformers library, specifies the t5-small pre-trained model, and then initializes both the tokenizer (to convert text to numerical IDs) and the model itself from that pre-trained version.\n",
        "\n"
      ],
      "metadata": {
        "id": "arCLeevDtRDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"translate English to German: Good morning\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "translation_ids = model.generate(inputs.input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
        "translation_text = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
        "print(\"Translation:\", translation_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUzNOH9YtA63",
        "outputId": "2e07ea6e-2704-48c6-e6f1-c03479292c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation: Guten Morgen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet initializes a T5 tokenizer and model to perform machine translation. It tokenizes the input text ('translate English to German: Good morning'), generates translation IDs using the pre-trained T5 model, and then decodes these IDs back into human-readable text, finally printing the translated German phrase."
      ],
      "metadata": {
        "id": "FYB7NB8RtpmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connection Test"
      ],
      "metadata": {
        "id": "Fm8n8Fi5Wmx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gemini"
      ],
      "metadata": {
        "id": "YsJcIHHEWpQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-genai\n"
      ],
      "metadata": {
        "id": "wovTVBpUWq-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "GCP_MODEL = 'gemini-2.5-flash'\n",
        "GOOGLE_API_KEY = 'your-key'\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "from google import genai\n",
        "\n",
        "client = genai.Client()\n",
        "response = client.models.generate_content(\n",
        "    model=GCP_MODEL,\n",
        "    contents=\"Explain large language models in one sentence.\"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "g21PsaJ6WrC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AWS"
      ],
      "metadata": {
        "id": "mJLciPCdWtCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n"
      ],
      "metadata": {
        "id": "vRglbVMSWuGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "AWS_REGION = 'us-east-1'\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = yourid\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = your-key\n",
        "os.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n",
        "\n",
        "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
        "\n",
        "import boto3\n",
        "import os\n",
        "print(\"Checking credentials...\")\n",
        "print(f\"Access Key ID: {os.environ.get('AWS_ACCESS_KEY_ID', 'NOT SET')}\")\n",
        "print(f\"Secret Key: {os.environ.get('AWS_SECRET_ACCESS_KEY', 'NOT SET')[:10]}... (hidden)\")\n",
        "print(f\"Region: {os.environ.get('AWS_DEFAULT_REGION', 'NOT SET')}\")\n",
        "sts = boto3.client('sts')\n",
        "identity = sts.get_caller_identity()\n",
        "print(\"\\n✅ Credentials are VALID!\")\n",
        "print(f\"Account: {identity['Account']}\")\n",
        "print(f\"User ARN: {identity['Arn']}\")"
      ],
      "metadata": {
        "id": "763mWGvAWusc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import boto3\n",
        "\n",
        "client = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)\n",
        "\n",
        "user_message = \"In single sentence, what are LLMs?\"\n",
        "conversation = [{\"role\": \"user\",\"content\": [{\"text\": user_message}]}]\n",
        "response = client.converse(\n",
        "    modelId=model_id,\n",
        "    messages=conversation,\n",
        "    inferenceConfig={\"maxTokens\": 512, \"temperature\": 0.5},\n",
        ")\n",
        "\n",
        "print(response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "529MvclkWzwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI"
      ],
      "metadata": {
        "id": "hqcaGtu6W6RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the API key from Colab secrets\n",
        "#OPENAI_API_KEY = \"\"#userdata.get('OPENAI_API_KEY')\n",
        "OPENAI_API_KEY = your-key\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "sys_msg = 'You are a GenAI expert. That will explain concepts to a layman audience'\n",
        "prompt_question = \"In a sentence, what is the difference from Deep Learning?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": sys_msg},\n",
        "        {\"role\": \"user\", \"content\": \"In a sentence, what are LLMs\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"LLMs are advanced artificial intelligence systems trained on vast amounts of text data to understand and generate human-like language.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_question}\n",
        "    ],\n",
        "    temperature=0.9,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "iT4EtZA1W7Oq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}