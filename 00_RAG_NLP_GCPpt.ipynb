{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcbjrrr/genai/blob/main/00_RAG_NLP_GCPpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fundamentos"
      ],
      "metadata": {
        "id": "skct5N00hxEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Natural Language Processing (NLP)"
      ],
      "metadata": {
        "id": "DHazxZdAiED1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Processamento de Linguagem Natural (NLP) é um campo multidisciplinar na interseção da ciência da computação, inteligência artificial (IA) e linguística. Seu objetivo principal é permitir que os computadores compreendam, interpretem e gerem linguagem humana de uma forma significativa e útil. Isso envolve o desenvolvimento de algoritmos e modelos para processar dados brutos de texto e fala, permitindo que as máquinas realizem tarefas como tradução, análise de sentimentos, sumarização de texto e assistentes de voz. O NLP é fundamental para a forma como as pessoas interagem com a tecnologia, tornando os sistemas mais intuitivos e capazes de lidar com a complexidade e as nuances da comunicação humana."
      ],
      "metadata": {
        "id": "CAi5srgIiIDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Neural Networks and Deep Learning"
      ],
      "metadata": {
        "id": "4cJpMa5xifIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A aprendizagem profunda, baseada em redes neurais e especificamente em Redes Neurais Recorrentes (RNNs), revolucionou o Processamento de Linguagem Natural (PLN) ao permitir que os computadores compreendam a natureza sequencial da linguagem humana. Variantes de RNN, como LSTM e GRUs, utilizam um estado oculto interno (memória) para manter o contexto ao longo de uma sequência de palavras, superando a limitação das redes neurais padrão. Essa capacidade permite que elas se destaquem em tarefas sequenciais, como tradução automática (usando modelos codificador-decodificador), modelagem de linguagem para geração de texto e análise de sentimentos. Embora a arquitetura Transformer mais recente tenha se tornado dominante, as RNNs estabeleceram a base crucial da aprendizagem profunda para lidar com a complexidade, o contexto e as dependências inerentes aos dados textuais.\n",
        "\n",
        "![](https://pbs.twimg.com/media/G55BA6oXIAA4x_y?format=jpg&name=900x900)"
      ],
      "metadata": {
        "id": "oaSIU4q8ivmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Feedforward Neural Network (FNN)"
      ],
      "metadata": {
        "id": "ijD10UByjeAM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6f57863"
      },
      "source": [
        "Uma rede neural de alimentação direta, frequentemente chamada de Rede Neural Feedforward (FNN), é o tipo mais simples de rede neural artificial. Em uma FNN, a informação se move em apenas uma direção — para frente — da camada de entrada, passando por quaisquer camadas ocultas, e finalmente chegando à camada de saída. Não há loops ou ciclos, o que significa que os dados fluem linearmente sem retornar ao início, tornando-as adequadas para tarefas como classificação e regressão, onde a entrada é mapeada diretamente para uma saída.\n",
        "\n",
        "![](https://pbs.twimg.com/media/G55BMjMWwAAFIVf?format=png&name=360x360)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recurrent Neural Networks (RNNs)"
      ],
      "metadata": {
        "id": "HYKeVK_Rkgar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma Rede Neural Recorrente (RNN) é um tipo de rede neural projetada especificamente para processar dados sequenciais, como texto, fala ou séries temporais. Ao contrário das Redes Feedforward, as RNNs possuem um loop que permite a passagem de informações de uma etapa da rede para a seguinte, efetivamente conferindo-lhes uma memória interna ou estado oculto. Essa memória permite que a rede considere o contexto dos elementos anteriores em uma sequência ao processar o elemento atual. Isso torna as RNNs particularmente adequadas para tarefas como processamento de linguagem natural (PLN), onde o significado de uma palavra depende fortemente das palavras que a precedem, permitindo que elas executem tarefas dependentes de sequência, como modelagem de linguagem e tradução automática.\n",
        "\n",
        "![](https://pbs.twimg.com/media/G55B_46WMAA12ts?format=png&name=360x360)"
      ],
      "metadata": {
        "id": "intJ8Hiik6Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "text = \"Deep learning, based on neural networks and specifically Recurrent Neural Networks (RNNs), revolutionized Natural Language Processing (NLP)\"\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_index = {char: i for i, char in enumerate(chars)}\n",
        "index_to_char = {i: char for i, char in enumerate(chars)}\n",
        "char_to_index"
      ],
      "metadata": {
        "id": "bi7Gfp9il92x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = text[0:0 + 3]\n",
        "label = text[3]\n",
        "print(seq,([char_to_index[char] for char in seq]))\n",
        "print(label,char_to_index[label])"
      ],
      "metadata": {
        "id": "nbBlpkMGnpuX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 3\n",
        "sequences = []\n",
        "labels = []\n",
        "for i in range(len(text) - seq_length):\n",
        "    seq = text[i:i + seq_length]\n",
        "    label = text[i + seq_length]\n",
        "    sequences.append([char_to_index[char] for char in seq])\n",
        "    labels.append(char_to_index[label])\n",
        "\n",
        "X = np.array(sequences)\n",
        "y = np.array(labels)\n",
        "print(y)\n",
        "X"
      ],
      "metadata": {
        "id": "KcPIXBHtmdQH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_one_hot = tf.one_hot(X, len(chars))\n",
        "y_one_hot = tf.one_hot(y, len(chars))\n",
        "y_one_hot"
      ],
      "metadata": {
        "id": "QRBM-ljFoaKN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(50, input_shape=(seq_length, len(chars)), activation='relu'))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_one_hot, y_one_hot, epochs=100)"
      ],
      "metadata": {
        "id": "dKnwRc7sol-r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_seq = \"Deep learn\"\n",
        "generated_text = start_seq\n",
        "x = np.array([[char_to_index[char] for char in generated_text[-seq_length:]]])\n",
        "print(x)\n",
        "x_one_hot = tf.one_hot(x, len(chars))\n",
        "prediction = model.predict(x_one_hot)\n",
        "print(prediction)\n",
        "next_index = np.argmax(prediction)\n",
        "print('i=',next_index)\n",
        "index_to_char[next_index]"
      ],
      "metadata": {
        "id": "GwidfmOMpDYr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(60):\n",
        "    x = np.array([[char_to_index[char] for char in generated_text[-seq_length:]]])\n",
        "    x_one_hot = tf.one_hot(x, len(chars))\n",
        "    prediction = model.predict(x_one_hot)\n",
        "    next_index = np.argmax(prediction)\n",
        "    next_char = index_to_char[next_index]\n",
        "    generated_text += next_char\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "LvuB1te8qJ2c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5 (Text-to-Text Transfer Transformer)"
      ],
      "metadata": {
        "id": "9bzucX34l88E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo T5 (Text-to-Text Transfer Transformer), desenvolvido pelo Google AI, revolucionou o PNL (Processamento de Linguagem Natural) ao introduzir uma estrutura unificada onde cada tarefa linguística é tratada como um problema de texto para texto. Construído sobre a estrutura codificador-decodificador da arquitetura Transformer, o T5 consegue lidar com diversas tarefas — incluindo tradução, sumarização, resposta a perguntas e classificação — simplesmente fornecendo à entrada um prefixo específico da tarefa (por exemplo, \"traduzir inglês para alemão: ...\") e recebendo a saída como texto simples. Essa consistência simplifica o design do modelo e permite que um único modelo pré-treinado alcance resultados de última geração em diversos benchmarks após o ajuste fino.\n",
        "\n",
        "![](https://pbs.twimg.com/media/G55IpcfW8AAYrY4?format=png&name=360x360)"
      ],
      "metadata": {
        "id": "La3VVyFIrih8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os Transformers são uma arquitetura de rede neural poderosa que utiliza um **mecanismo de atenção** para ponderar a importância de diferentes partes dos dados de entrada, permitindo-lhes processar sequências em paralelo e capturar eficientemente dependências de longo alcance, tornando-se a base para modelos de linguagem modernos de grande escala como o BERT e o GPT.\n",
        "\n",
        "![](https://pbs.twimg.com/media/G55JZMnXAAAgub2?format=jpg&name=900x900)"
      ],
      "metadata": {
        "id": "jBPBg5ZasS3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers torch sentencepiece"
      ],
      "metadata": {
        "id": "LOirAkknstt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "model_name = \"t5-small\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "SwgYUN9Eripp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de linguagem T5 para tarefas de texto para texto. Importa os componentes necessários da biblioteca transformers, especifica o modelo pré-treinado t5-small e, em seguida, inicializa tanto o tokenizador (para converter texto em IDs numéricos) quanto o próprio modelo a partir dessa versão pré-treinada.\n",
        "\n"
      ],
      "metadata": {
        "id": "arCLeevDtRDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"translate English to German: Good morning\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\") #PyTorch\n",
        "inputs"
      ],
      "metadata": {
        "id": "aLgiypk5tFGw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation_ids = model.generate(inputs.input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
        "print(translation_ids)\n",
        "translation_text = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
        "print(\"Translation:\", translation_text)"
      ],
      "metadata": {
        "id": "IUzNOH9YtA63"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um tokenizador e modelo T5 para realizar tradução automática. Ele tokeniza o texto de entrada ('traduzir inglês para alemão: Bom dia'), gera IDs de tradução usando o modelo T5 pré-treinado e, em seguida, decodifica esses IDs de volta para um texto legível por humanos, finalmente imprimindo a frase traduzida em alemão."
      ],
      "metadata": {
        "id": "FYB7NB8RtpmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "id": "IOEuv8olV11H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "GCP_MODEL = 'gemini-2.5-flash'\n",
        "GOOGLE_API_KEY=your-key\n",
        "#os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel(GCP_MODEL)\n",
        "\n",
        "response = model.generate_content(\n",
        "    contents=\"Explain large language models in one sentence.\"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "Pc3CbOicUQsJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AWS"
      ],
      "metadata": {
        "id": "1XilNvHDUwx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "id": "hwImZ7gSU6p1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "AWS_REGION = 'us-east-1'\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = yourid\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = your-key\n",
        "os.environ['AWS_DEFAULT_REGION'] = AWS_REGION\n",
        "\n",
        "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
        "\n",
        "import boto3\n",
        "import os\n",
        "print(\"Checking credentials...\")\n",
        "print(f\"Access Key ID: {os.environ.get('AWS_ACCESS_KEY_ID', 'NOT SET')}\")\n",
        "print(f\"Secret Key: {os.environ.get('AWS_SECRET_ACCESS_KEY', 'NOT SET')[:10]}... (hidden)\")\n",
        "print(f\"Region: {os.environ.get('AWS_DEFAULT_REGION', 'NOT SET')}\")\n",
        "sts = boto3.client('sts')\n",
        "identity = sts.get_caller_identity()\n",
        "print(\"\\n✅ Credentials are VALID!\")\n",
        "print(f\"Account: {identity['Account']}\")\n",
        "print(f\"User ARN: {identity['Arn']}\")"
      ],
      "metadata": {
        "id": "JBXWi5RiUyo7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import boto3\n",
        "\n",
        "client = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)\n",
        "\n",
        "user_message = \"In single sentence, what are LLMs?\"\n",
        "conversation = [{\"role\": \"user\",\"content\": [{\"text\": user_message}]}]\n",
        "response = client.converse(\n",
        "    modelId=model_id,\n",
        "    messages=conversation,\n",
        "    inferenceConfig={\"maxTokens\": 512, \"temperature\": 0.5},\n",
        ")\n",
        "\n",
        "print(response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "Fnm1AZIOU1Ey"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI"
      ],
      "metadata": {
        "id": "Loja3RVDbcwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the API key from Colab secrets\n",
        "#OPENAI_API_KEY = \"\"#userdata.get('OPENAI_API_KEY')\n",
        "OPENAI_API_KEY = your-key\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "sys_msg = 'You are a GenAI expert. That will explain concepts to a layman audience'\n",
        "prompt_question = \"In a sentence, what is the difference from Deep Learning?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": sys_msg},\n",
        "        {\"role\": \"user\", \"content\": \"In a sentence, what are LLMs\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"LLMs are advanced artificial intelligence systems trained on vast amounts of text data to understand and generate human-like language.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_question}\n",
        "    ],\n",
        "    temperature=0.9,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "s8bYZeslbeTN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gRg6ayNWQ7L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atividade: Tradução de Máquina\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dFrOT_fRZtvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Atividade: Tradução de Máquina\n",
        "\n",
        "Esta tarefa tem como objetivo que os alunos explorem e comparem a eficácia de diferentes ferramentas de tradução na conversão de textos que contêm terminologia específica de domínio de negócio. O aluno deve selecionar um texto-fonte curto (ate 5 paginas) rico em jargões técnicos de uma área de negócio, e em seguida, utilizar três ferramentas de tradução distintas, para traduzir esse texto para duas línguas de destino. A etapa crucial será a análise comparativa dos resultados, onde o foco deve estar na precisão terminológica dos termos de negócio, na coerência e na fluidez do texto traduzido. Finalmente, o aluno deverá elaborar um breve relatório que detalhe qual ferramenta demonstrou a melhor (e pior) performance na manutenção do significado e da precisão técnica do vocabulário especializado.\n",
        "\n",
        "Desafío:  Como um desafio extra, reflita sobre as diferenças entre traduzir um texto corrido e traduzir o conteúdo de uma apresentação de slides (PowerPoint ou Google Slides)."
      ],
      "metadata": {
        "id": "Ph1EibR4ZwLt"
      }
    }
  ]
}