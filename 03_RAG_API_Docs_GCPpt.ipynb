{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcbjrrr/genai/blob/main/03_RAG_API_Docs_GCPpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG: API Integration"
      ],
      "metadata": {
        "id": "IEBS7dQOcw_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector DB and Embeddings"
      ],
      "metadata": {
        "id": "D8ZyGl7ibU6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As arquiteturas RAG aprimoram grandes modelos de linguagem (LLMs) fornecendo-lhes informações externas relevantes antes de gerar uma resposta. Em vez de depender exclusivamente de seu conhecimento pré-treinado, os sistemas RAG primeiro recuperam dados pertinentes de uma base de conhecimento, geralmente um banco de dados vetorial, com base na consulta do usuário. Esse contexto recuperado é então inserido no LLM juntamente com a consulta original, permitindo que o modelo produza respostas mais precisas, factuais e atualizadas. Esse processo ajuda a mitigar limitações comuns dos LLMs, como alucinações e informações desatualizadas, fundamentando o conteúdo gerado em fontes verificáveis.\n",
        "\n",
        "No núcleo do RAG estão os **bancos de dados vetoriais** e os embeddings. Trata-se de um banco de dados especializado, otimizado para armazenar e consultar vetores de alta dimensionalidade, conhecidos como embeddings. Ao contrário dos bancos de dados tradicionais que lidam diretamente com dados estruturados ou documentos, os bancos de dados vetoriais se destacam na busca por vetores semanticamente semelhantes a um vetor de consulta específico. Essa eficiência na busca por similaridade é vital para o RAG, permitindo a rápida identificação de documentos relevantes em uma vasta coleção.\n",
        "\n",
        "Os **embeddings** são representações numéricas de dados (como texto, imagens ou áudio) em um espaço multidimensional. Essas representações são criadas por modelos de aprendizado de máquina, onde o significado semântico dos dados é codificado pela proximidade de seus vetores nesse espaço. Por exemplo, trechos de texto com significados semelhantes terão vetores de embedding mais próximos, permitindo que os computadores entendam e comparem o conteúdo com base em seu contexto semântico.\n",
        "\n",
        "![](https://pbs.twimg.com/media/G5Z7EdLXMAA-BSI?format=jpg&name=small)"
      ],
      "metadata": {
        "id": "ZMc60G-kbVW-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrWth_Z7oWfC",
        "outputId": "0c9a70f4-669c-4fd6-ac92-5eb76d35fbf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.2/471.2 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-core langchain-chroma langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "import os\n",
        "API_KEY = 'yourkey'\n",
        "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "ns={'atom': 'http://www.w3.org/2005/Atom'}\n",
        "embed_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "e=embeddings.embed_documents(\"Python is the best to do LLM\")\n",
        "print(e[-1])\n",
        "len(e[-1])"
      ],
      "metadata": {
        "id": "anz6FOA_ev8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "https://export.arxiv.org/api/query?search_query=cat:cs.AI+AND+abs:java&start=0&max_results=1\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "GEOhof0D5-E7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "  <entry>\n",
        "    <id>http://arxiv.org/abs/1908.01031v1</id>\n",
        "    <updated>2019-08-02T19:53:46Z</updated>\n",
        "    <published>2019-08-02T19:53:46Z</published>\n",
        "    <title>RuleKit: A Comprehensive Suite for Rule-Based Learning</title>\n",
        "    <summary>  Rule-based models are often used for data analysis as they combine\n",
        "interpretability with predictive power. We present RuleKit, a versatile tool\n",
        "for rule learning. Based on a sequential covering induction algorithm, it is\n",
        "suitable for classification, regression, and survival problems. The presence of\n",
        "a user-guided induction facilitates verifying hypotheses concerning data\n",
        "dependencies which are expected or of interest. The powerful and flexible\n",
        "experimental environment allows straightforward investigation of different\n",
        "induction schemes. The analysis can be performed in batch mode, through\n",
        "RapidMiner plug-in, or R package. A documented Java API is also provided for\n",
        "convenience. The software is publicly available at GitHub under GNU AGPL-3.0\n",
        "license.\n",
        "```\n"
      ],
      "metadata": {
        "id": "7tKPjD8ho4Uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain"
      ],
      "metadata": {
        "id": "hRG6tfRLdKln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain é um framework projetado para simplificar a criação de aplicações usando modelos de linguagem de grande escala (LLMs). Ele fornece um conjunto de ferramentas modular e flexível que permite aos desenvolvedores encadear vários componentes, como LLMs, modelos de prompts e outras ferramentas, para construir aplicações complexas e poderosas. Sua ideia central é possibilitar o desenvolvimento de aplicações de raciocínio sensíveis ao contexto, capazes de conectar LLMs a outras fontes de dados e agentes, facilitando a construção de sistemas de IA sofisticados.\n",
        "\n",
        "![](https://pbs.twimg.com/media/G5Z6323WAAAE_Se?format=jpg&name=small)"
      ],
      "metadata": {
        "id": "ZE-xTCiJdL7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "def doc_from_xml(entry,a_section, a_theme):\n",
        "  title_element = entry.find('atom:title', ns)\n",
        "  title = title_element.text.strip() if title_element is not None and title_element.text is not None else \"No Title\"\n",
        "\n",
        "  published_element = entry.find('atom:published', ns)\n",
        "  published = published_element.text.strip() if published_element is not None and published_element.text is not None else \"No Published Date\"\n",
        "\n",
        "  id_element = entry.find('atom:id', ns)\n",
        "  entry_id = id_element.text.strip() if id_element is not None and id_element.text is not None else \"No Entry ID\"\n",
        "\n",
        "  summary_element = entry.find('atom:summary', ns)\n",
        "  summary = summary_element.text.strip() if summary_element is not None and summary_element.text is not None else \"No Summary\"\n",
        "\n",
        "  authors = []\n",
        "  for author_element in entry.findall('atom:author', ns):\n",
        "      name_element = author_element.find('atom:name', ns)\n",
        "      if name_element is not None and name_element.text is not None:\n",
        "          authors.append(name_element.text.strip())\n",
        "  author_names = \", \".join(authors) if authors else \"No Authors\"\n",
        "  doc = Document(\n",
        "    page_content=summary,\n",
        "    metadata={\n",
        "        \"section\": a_section,\n",
        "        \"theme\": a_theme,\n",
        "        \"title\": title,\n",
        "        \"published\": published,\n",
        "        \"author_names\": author_names,\n",
        "        \"entry_id\": entry_id})\n",
        "  return doc\n"
      ],
      "metadata": {
        "id": "8qD5zE1Jo3Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "xEx = \"\"\"<?xml version='1.0' encoding='UTF-8'?>\n",
        "<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n",
        "  <id>https://arxiv.org/api/9VsrqgLtBqizNdn2qfxBueBGmP4</id>\n",
        "  <title>arXiv Query: search_query=cat:cs.AI AND abs:java&amp;id_list=&amp;start=0&amp;max_results=1</title>\n",
        "  <updated>2025-12-08T21:40:34Z</updated>\n",
        "  <link href=\"https://arxiv.org/api/query?search_query=cat:cs.AI+AND+abs:java&amp;start=0&amp;max_results=1&amp;id_list=\" type=\"application/atom+xml\"/>\n",
        "  <opensearch:itemsPerPage>1</opensearch:itemsPerPage>\n",
        "  <opensearch:totalResults>273</opensearch:totalResults>\n",
        "  <opensearch:startIndex>0</opensearch:startIndex>\n",
        "  <entry>\n",
        "    <id>http://arxiv.org/abs/2407.03941v2</id>\n",
        "    <title>Narrow Transformer: StarCoder-Based Java-LM For Desktop</title>\n",
        "    <updated>2024-09-07T11:40:47Z</updated>\n",
        "    <link href=\"https://arxiv.org/abs/2407.03941v2\" rel=\"alternate\" type=\"text/html\"/>\n",
        "    <link href=\"https://arxiv.org/pdf/2407.03941v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
        "    <summary>This paper presents NT-Java-1.1B, an open-source specialized code language model built on StarCoderBase-1.1B, designed for coding tasks in Java programming. NT-Java-1.1B achieves state-of-the-art performance, surpassing its base model and majority of other models of similar size on MultiPL-E Java code benchmark. While there have been studies on extending large, generic pre-trained models to improve proficiency in specific programming languages like Python, similar investigations on small code models for other programming languages are lacking. Large code models require specialized hardware like GPUs for inference, highlighting the need for research into building small code models that can be deployed on developer desktops. This paper addresses this research gap by focusing on the development of a small Java code model, NT-Java-1.1B, and its quantized versions, which performs comparably to open models around 1.1B on MultiPL-E Java code benchmarks, making them ideal for desktop deployment. This paper establishes the foundation for specialized models across languages and sizes for a family of NT Models.</summary>\n",
        "    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
        "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
        "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
        "    <published>2024-07-04T13:54:24Z</published>\n",
        "    <arxiv:comment>Updated Authors list</arxiv:comment>\n",
        "    <arxiv:primary_category term=\"cs.SE\"/>\n",
        "    <author>\n",
        "      <name>Kamalkumar Rathinasamy</name>\n",
        "    </author>\n",
        "  </entry>\n",
        "</feed>\n",
        "\"\"\"\n",
        "print(doc_from_xml(ET.fromstring(xEx),'s1','t1'))"
      ],
      "metadata": {
        "id": "OB4LrV7B1KF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Document(\n",
        "```\n",
        "```\n",
        "metadata={'section': 'cs.AI', 'theme': 'java', 'title': 'RuleKit: A Comprehensive Suite for Rule-Based Learning', 'published': '2019-08-02T19:53:46Z', 'author_names': 'Adam Gudyś, Marek Sikora, Łukasz Wróbel', 'entry_id': 'http://arxiv.org/abs/1908.01031v1'},\n",
        "```\n",
        "\n",
        "```\n",
        "page_content='Rule-based models are often used for data analysis as they combine\\ninterpretability with predictive power. We present RuleKit, a versatile tool\\nfor rule learning. Based on a sequential covering induction algorithm, it is\\nsuitable for classification, regression, and survival problems. The presence of\\na user-guided induction facilitates verifying hypotheses concerning data\\ndependencies which are expected or of interest. The powerful and flexible\\nexperimental environment allows straightforward investigation of different\\ninduction schemes. The analysis can be performed in batch mode, through\\nRapidMiner plug-in, or R package. A documented Java API is also provided for\\nconvenience. The software is publicly available at GitHub under GNU AGPL-3.0\\nlicense.')\n",
        "```\n",
        "```\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "kyDilp99pMkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_documents_from_xml(root, section, theme, vectordb):\n",
        "  documents_from_xml = []\n",
        "  for entry in root.findall('atom:entry', ns):\n",
        "    doc = doc_from_xml(entry,section, theme)\n",
        "    documents_from_xml.append(doc)\n",
        "    print('- ',doc.metadata['title'])\n",
        "  vectordb.add_documents(documents_from_xml)\n",
        "  return documents_from_xml"
      ],
      "metadata": {
        "id": "fWc8gQQvqIli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O **ChromaDB** é um banco de dados de embeddings de código aberto projetado para facilitar a criação de aplicações LLM. Ele permite armazenar, consultar e gerenciar embeddings, possibilitando seu uso em diversas tarefas, como busca semântica, resposta a perguntas e sistemas de recomendação. Sua simplicidade de uso o torna uma escolha popular para aplicações RAG (Retrieval-Augmented Generation) devido à sua fácil integração com frameworks como o LangChain."
      ],
      "metadata": {
        "id": "8RTHb6kshbsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_chroma"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mP9LFepthpxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "MAIN_THEME = 'java'\n",
        "MAIN_SECTION = 'econ.GN'\n",
        "sections = ['econ.GN','cs.AI']\n",
        "qty=100\n",
        "vectorstore = Chroma(embedding_function=embed_model,persist_directory='./vectordb')\n",
        "print(\"Empty ChromaDB collection created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r4285OKpNCg",
        "outputId": "60aae387-7abe-43e5-9f4d-1fbdbf11173f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty ChromaDB collection created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "for sec in sections:\n",
        "  print(f\"Processing section: {sec}\")\n",
        "  api_url = f'https://export.arxiv.org/api/query?search_query=cat:{sec}+AND+abs:{MAIN_THEME}&start=0&max_results={qty}'\n",
        "  response = requests.get(api_url)\n",
        "  xml_data = response.text\n",
        "  root = ET.fromstring(xml_data)\n",
        "  print(len(root.findall('atom:entry', ns)))\n",
        "  docs = store_documents_from_xml(root, sec, MAIN_THEME, vectorstore)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YBE1FDiYppOb",
        "outputId": "6f734269-8b25-4aed-9c49-db5245596606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section: econ.GN\n",
            "4\n",
            "-  The Response of Farmer Welfares Amidst Food Prices Shock and Inflation\n",
            "  in the Province of East Java\n",
            "-  Econometric model of children participation in family dairy farming in\n",
            "  the center of dairy farming, West Java Province, Indonesia\n",
            "-  Strategic ESG-Driven Human Resource Practices: Transforming Employee\n",
            "  Management for Sustainable Organizational Growth\n",
            "-  CostMAP: An open-source software package for developing cost surfaces\n",
            "Processing section: cs.AI\n",
            "100\n",
            "-  Python Agent in Ludii\n",
            "-  SATEN: An Object-Oriented Web-Based Revision and Extraction Engine\n",
            "-  Release ZERO.0.1 of package RefereeToolbox\n",
            "-  BoolVar/PB v1.0, a java library for translating pseudo-Boolean\n",
            "  constraints into CNF formulae\n",
            "-  XCSP3-core: A Format for Representing Constraint\n",
            "  Satisfaction/Optimization Problems\n",
            "-  GeoGebra Tools with Proof Capabilities\n",
            "-  From Code to Play: Benchmarking Program Search for Games Using Large\n",
            "  Language Models\n",
            "-  \"Model and Run\" Constraint Networks with a MILP Engine\n",
            "-  ADAPQUEST: A Software for Web-Based Adaptive Questionnaires based on\n",
            "  Bayesian Networks\n",
            "-  Uncertainty in Ontologies: Dempster-Shafer Theory for Data Fusion\n",
            "  Applications\n",
            "-  An Intelligent Approach for Negotiating between chains in Supply Chain\n",
            "  Management Systems\n",
            "-  Implementing a Wall-In Building Placement in StarCraft with Declarative\n",
            "  Programming\n",
            "-  Knowledge Representation on the Web revisited: Tools for Prototype Based\n",
            "  Ontologies\n",
            "-  Extending Modular Semantics for Bipolar Weighted Argumentation\n",
            "  (Technical Report)\n",
            "-  Design and Implementation of TAG: A Tabletop Games Framework\n",
            "-  ACE, a generic constraint solver\n",
            "-  ConArg: a Tool to Solve (Weighted) Abstract Argumentation Frameworks\n",
            "  with (Soft) Constraints\n",
            "-  A Comparison of Public Causal Search Packages on Linear, Gaussian Data\n",
            "  with No Latent Variables\n",
            "-  A Tutorial for Weighted Bipolar Argumentation with Continuous Dynamical\n",
            "  Systems and the Java Library Attractor\n",
            "-  Adaptive Task Assignment in Online Learning Environments\n",
            "-  Optimizing Heuristics for Tableau-based OWL Reasoners\n",
            "-  CREPO: An Open Repository to Benchmark Credal Network Algorithms\n",
            "-  Worst-Case Analysis is Maximum-A-Posteriori Estimation\n",
            "-  CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding\n",
            "  and Execution\n",
            "-  Handling irresolvable conflicts in the Semantic Web: an RDF-based\n",
            "  conflict-tolerant version of the Deontic Traditional Scheme\n",
            "-  A Language Model of Java Methods with Train/Test Deduplication\n",
            "-  Automated Testing of COBOL to Java Transformation\n",
            "-  Aplib: Tactical Programming of Intelligent Agents\n",
            "-  Static Program Analysis Guided LLM Based Unit Test Generation\n",
            "-  Marvin: Semantic annotation using multiple knowledge sources\n",
            "-  Comparison between CPBPV, ESC/Java, CBMC, Blast, EUREKA and Why for\n",
            "  Bounded Program Verification\n",
            "-  Achieving Fairness in DareFightingICE Agents Evaluation Through a Delay\n",
            "  Mechanism\n",
            "-  JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating\n",
            "  Large Language Models\n",
            "-  Authoring Worked Examples for Java Programming with Human-AI\n",
            "  Collaboration\n",
            "-  Human-AI Co-Creation of Worked Examples for Programming Classes\n",
            "-  CSA-Trans: Code Structure Aware Transformer for AST\n",
            "-  Automated Validation of COBOL to Java Transformation\n",
            "-  RAILS: Retrieval-Augmented Intelligence for Learning Software\n",
            "  Development\n",
            "-  Ontology Temporal Evolution for Multi-Entity Bayesian Networks under\n",
            "  Exogenous and Endogenous Semantic Updating\n",
            "-  jsdp: a Java Stochastic DP Library\n",
            "-  Intelligent search strategies based on adaptive Constraint Handling\n",
            "  Rules\n",
            "-  CUBETESTERAI: Automated JUnit Test Generation using the LLaMA Model\n",
            "-  From Empirical Evaluation to Context-Aware Enhancement: Repairing\n",
            "  Regression Errors with LLMs\n",
            "-  Probabilistic Graphical Models on Multi-Core CPUs using Java 8\n",
            "-  Software Vulnerability Prediction Knowledge Transferring Between\n",
            "  Programming Languages\n",
            "-  Py-Tetrad and RPy-Tetrad: A New Python Interface with R Support for\n",
            "  Tetrad Causal Search\n",
            "-  Can Programming Languages Boost Each Other via Instruction Tuning?\n",
            "-  STraceBERT: Source Code Retrieval using Semantic Application Traces\n",
            "-  Code Reborn AI-Driven Legacy Systems Modernization from COBOL to Java\n",
            "-  CoTran: An LLM-based Code Translator using Reinforcement Learning with\n",
            "  Feedback from Compiler and Symbolic Execution\n",
            "-  Using Java Geometry Expert as Guide in the Preparations for Math\n",
            "  Contests\n",
            "-  Narrow Transformer: StarCoder-Based Java-LM For Desktop\n",
            "-  Extending Object-Oriented Languages by Declarative Specifications of\n",
            "  Complex Objects using Answer-Set Programming\n",
            "-  New Heuristics for Interfacing Human Motor System using Brain Waves\n",
            "-  Testing Global Constraints\n",
            "-  Mining Software Components from Object-Oriented APIs\n",
            "-  Opti Code Pro: A Heuristic Search-based Approach to Code Refactoring\n",
            "-  Quality Evaluation of COBOL to Java Code Transformation\n",
            "-  How Effective Are Neural Networks for Fixing Security Vulnerabilities\n",
            "-  Syntax and Domain Aware Model for Unsupervised Program Translation\n",
            "-  CoDesc: A Large Code-Description Parallel Dataset\n",
            "-  A comprehensible analysis of the efficacy of Ensemble Models for Bug\n",
            "  Prediction\n",
            "-  Studying Vulnerable Code Entities in R\n",
            "-  GitHub Copilot: the perfect Code compLeeter?\n",
            "-  ScenEval: A Benchmark for Scenario-Based Evaluation of Code Generation\n",
            "-  Evaluating ChatGPT-3.5 Efficiency in Solving Coding Problems of\n",
            "  Different Complexity Levels: An Empirical Analysis\n",
            "-  I Can Find You in Seconds! Leveraging Large Language Models for Code\n",
            "  Authorship Attribution\n",
            "-  Skeleton-Guided-Translation: A Benchmarking Framework for Code\n",
            "  Repository Translation with Fine-Grained Quality Evaluation\n",
            "-  LLM-Driven Collaborative Model for Untangling Commits via Explicit and\n",
            "  Implicit Dependency Reasoning\n",
            "-  MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection\n",
            "  Covering Multiple Languages, Models,Prompts, and Scenarios\n",
            "-  Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large\n",
            "  Language Models\n",
            "-  FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration\n",
            "-  Enhancing Neural Code Representation with Additional Context\n",
            "-  LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test\n",
            "  Generation\n",
            "-  CTBNCToolkit: Continuous Time Bayesian Network Classifier Toolkit\n",
            "-  Grounded Discovery of Coordinate Term Relationships between Software\n",
            "  Entities\n",
            "-  Search4Code: Code Search Intent Classification Using Weak Supervision\n",
            "-  LeoTask: a fast, flexible and reliable framework for computational\n",
            "  research\n",
            "-  Universal Feature Selection Tool (UniFeat): An Open-Source Tool for\n",
            "  Dimensionality Reduction\n",
            "-  NLML--a Markup Language to Describe the Unlimited English Grammar\n",
            "-  Combining Relational Algebra, SQL, Constraint Modelling, and Local\n",
            "  Search\n",
            "-  Mouse Simulation Using Two Coloured Tapes\n",
            "-  Adaptive Measurement-Based Policy-Driven QoS Management with\n",
            "  Fuzzy-Rule-based Resource Allocation\n",
            "-  Building a Truly Distributed Constraint Solver with JADE\n",
            "-  TrustyAI Explainability Toolkit\n",
            "-  CodeS: Towards Code Model Generalization Under Distribution Shift\n",
            "-  On the Eve of True Explainability for OWL Ontologies: Description Logic\n",
            "  Proofs with Evee and Evonne (Extended Version)\n",
            "-  OctoPack: Instruction Tuning Code Large Language Models\n",
            "-  The Behavior of Large Language Models When Prompted to Generate Code\n",
            "  Explanations\n",
            "-  Examination of Code generated by Large Language Models\n",
            "-  The Potential of LLMs in Automating Software Testing: From Generation to\n",
            "  Reporting\n",
            "-  Fine-tuning LLaMA 2 interference: a comparative study of language\n",
            "  implementations for optimal efficiency\n",
            "-  Marking Code Without Breaking It: Code Watermarking for Detecting\n",
            "  LLM-Generated Code\n",
            "-  EvoGPT: Enhancing Test Suite Robustness via LLM-Based Generation and\n",
            "  Genetic Optimization\n",
            "-  GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On\n",
            "  Git\n",
            "-  A Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods for\n",
            "  the Security of Code LLMs\n",
            "-  Investigating the Efficacy of Large Language Models for Code Clone\n",
            "  Detection\n",
            "-  SWE-bench-java: A GitHub Issue Resolving Benchmark for Java\n",
            "-  Generating executable oracles to check conformance of client code to\n",
            "  requirements of JDK Javadocs using LLMs\n",
            "-  RuleKit: A Comprehensive Suite for Rule-Based Learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMW5MvTgrMky",
        "outputId": "d833dd38-efe7-40ba-ec2d-29df60ca37d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'section': 'cs.AI', 'theme': 'java', 'title': 'RuleKit: A Comprehensive Suite for Rule-Based Learning', 'published': '2019-08-02T19:53:46Z', 'author_names': 'Adam Gudyś, Marek Sikora, Łukasz Wróbel', 'entry_id': 'http://arxiv.org/abs/1908.01031v1'}, page_content='Rule-based models are often used for data analysis as they combine\\ninterpretability with predictive power. We present RuleKit, a versatile tool\\nfor rule learning. Based on a sequential covering induction algorithm, it is\\nsuitable for classification, regression, and survival problems. The presence of\\na user-guided induction facilitates verifying hypotheses concerning data\\ndependencies which are expected or of interest. The powerful and flexible\\nexperimental environment allows straightforward investigation of different\\ninduction schemes. The analysis can be performed in batch mode, through\\nRapidMiner plug-in, or R package. A documented Java API is also provided for\\nconvenience. The software is publicly available at GitHub under GNU AGPL-3.0\\nlicense.')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query_vector_db(db,q,k,sec,th):\n",
        "  filter_criteria = 'None'\n",
        "  if sec:\n",
        "    filter_criteria = {\"$and\": [{\"section\": sec},{\"theme\": th}]}\n",
        "    results = db.similarity_search_with_score(q, k=k, filter=filter_criteria)\n",
        "  else:\n",
        "    results = db.similarity_search_with_score(q, k=k)\n",
        "  print(f\"Filtered hybrid search results for query: '{q}' (k={k}) with filter {filter_criteria}\")\n",
        "  if results:\n",
        "      for doc, score in results:\n",
        "          print(\"-\" * 50)\n",
        "          print(f'Score: {score}')\n",
        "          print(f\"Document Content: {doc.page_content}\")\n",
        "          print(f\"Document Metadata: {doc.metadata}\")\n",
        "          return doc\n",
        "  else:\n",
        "      print(\"No documents found matching the filter and query.\")"
      ],
      "metadata": {
        "id": "tsoumW2ysIh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#query = \"agricultural/farming province\"\n",
        "#econ.GN\n",
        "qq = input('Enter your query: ')\n",
        "ss = input('Enter your section: ')\n",
        "tt = None\n",
        "if ss is not None and ss != '':\n",
        "  tt = input('Enter your theme: ')\n",
        "query_vector_db(vectorstore,qq,5,ss,tt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn-3kpGysqpO",
        "outputId": "cb85ca20-b61c-4df6-887a-c251fd20d28f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: agricultural/farming province\n",
            "Enter your section: econ.GN\n",
            "Enter your theme: java\n",
            "Filtered hybrid search results for query: 'agricultural/farming province' (k=5) with filter {'$and': [{'section': 'econ.GN'}, {'theme': 'java'}]}\n",
            "--------------------------------------------------\n",
            "Score: 0.7361844778060913\n",
            "Document Content: Price uncertainty in food commodities can create uncertainty for farmers and\n",
            "potentially negatively impact the level of farmer household well-being. On the\n",
            "other hand, the agriculture sector in the province of East Java has greatly\n",
            "contributed to East Java's economy. This paper analyses the response of farmer\n",
            "welfare through farmer exchange values amidst fluctuation shock of food needed\n",
            "prices and inflation level in the east java province. The research method of\n",
            "this paper employs the impulse response function of the Bayesian Vector\n",
            "Autoregressive (BVAR) model by using time series secondary data from May 2017\n",
            "until December 2023. This paper finds that the shock that happens to aggregate\n",
            "food prices can increase farmer exchange values even though the shock to the\n",
            "inflation level has reduced farmer exchange values and increased aggregate food\n",
            "prices.\n",
            "Document Metadata: {'section': 'econ.GN', 'entry_id': 'http://arxiv.org/abs/2501.08601v1', 'author_names': 'Moh. Hairus Zaman, Diah Wahyuningsih, Ris Yuwono Yudo Nugroho', 'title': 'The Response of Farmer Welfares Amidst Food Prices Shock and Inflation\\n  in the Province of East Java', 'published': '2025-01-15T05:57:46Z', 'theme': 'java'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id='4c2ad76e-fbf2-4938-b96f-31f2a4a7826d', metadata={'section': 'econ.GN', 'entry_id': 'http://arxiv.org/abs/2501.08601v1', 'author_names': 'Moh. Hairus Zaman, Diah Wahyuningsih, Ris Yuwono Yudo Nugroho', 'title': 'The Response of Farmer Welfares Amidst Food Prices Shock and Inflation\\n  in the Province of East Java', 'published': '2025-01-15T05:57:46Z', 'theme': 'java'}, page_content=\"Price uncertainty in food commodities can create uncertainty for farmers and\\npotentially negatively impact the level of farmer household well-being. On the\\nother hand, the agriculture sector in the province of East Java has greatly\\ncontributed to East Java's economy. This paper analyses the response of farmer\\nwelfare through farmer exchange values amidst fluctuation shock of food needed\\nprices and inflation level in the east java province. The research method of\\nthis paper employs the impulse response function of the Bayesian Vector\\nAutoregressive (BVAR) model by using time series secondary data from May 2017\\nuntil December 2023. This paper finds that the shock that happens to aggregate\\nfood prices can increase farmer exchange values even though the shock to the\\ninflation level has reduced farmer exchange values and increased aggregate food\\nprices.\")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-genai"
      ],
      "metadata": {
        "id": "xdmaeZ2s025I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#query = \"agricultural/farming province\"\n",
        "#econ.GN\n",
        "#what would be a future work\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import os\n",
        "GCP_MODEL = 'gemini-2.5-flash'\n",
        "GOOGLE_API_KEY = 'yourkey'\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "qq = input('Enter your Topic: ')\n",
        "ss = input('Enter your Section: ')\n",
        "tt = None\n",
        "if ss is not None and ss != '':\n",
        "  tt = input('Enter your Theme: ')\n",
        "resp = query_vector_db(vectorstore,qq,1,ss,tt)\n",
        "\n",
        "PROMPT='''\n",
        "Please answer the question {q} regarding the paper abstract below:\n",
        "{abs}\n",
        "'''\n",
        "question = input('Enter your question/prompt: ')\n",
        "p =PROMPT.format(q=question,abs=resp.page_content)\n",
        "print(p)\n",
        "client = genai.Client()\n",
        "config = types.GenerateContentConfig(\n",
        "    system_instruction='You are scientific research assistant',\n",
        "    temperature=0.9)\n",
        "chat = client.chats.create(model=GCP_MODEL,config=config)\n",
        "response = chat.send_message(question)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIPMr-ubkTzx",
        "outputId": "92690cc3-8971-4416-c84d-f13ac4672dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Topic: agricultural/farming province\n",
            "Enter your Section: econ.GN\n",
            "Enter your Theme: java\n",
            "Filtered hybrid search results for query: 'agricultural/farming province' (k=1) with filter {'$and': [{'section': 'econ.GN'}, {'theme': 'java'}]}\n",
            "--------------------------------------------------\n",
            "Score: 0.7361844778060913\n",
            "Document Content: Price uncertainty in food commodities can create uncertainty for farmers and\n",
            "potentially negatively impact the level of farmer household well-being. On the\n",
            "other hand, the agriculture sector in the province of East Java has greatly\n",
            "contributed to East Java's economy. This paper analyses the response of farmer\n",
            "welfare through farmer exchange values amidst fluctuation shock of food needed\n",
            "prices and inflation level in the east java province. The research method of\n",
            "this paper employs the impulse response function of the Bayesian Vector\n",
            "Autoregressive (BVAR) model by using time series secondary data from May 2017\n",
            "until December 2023. This paper finds that the shock that happens to aggregate\n",
            "food prices can increase farmer exchange values even though the shock to the\n",
            "inflation level has reduced farmer exchange values and increased aggregate food\n",
            "prices.\n",
            "Document Metadata: {'entry_id': 'http://arxiv.org/abs/2501.08601v1', 'theme': 'java', 'published': '2025-01-15T05:57:46Z', 'section': 'econ.GN', 'title': 'The Response of Farmer Welfares Amidst Food Prices Shock and Inflation\\n  in the Province of East Java', 'author_names': 'Moh. Hairus Zaman, Diah Wahyuningsih, Ris Yuwono Yudo Nugroho'}\n",
            "Enter your question/prompt: what would be a future work\n",
            "\n",
            "You are scientific research assistant.\n",
            "Please answer the question what would be a future work regarding the paper abstract below:\n",
            "Price uncertainty in food commodities can create uncertainty for farmers and\n",
            "potentially negatively impact the level of farmer household well-being. On the\n",
            "other hand, the agriculture sector in the province of East Java has greatly\n",
            "contributed to East Java's economy. This paper analyses the response of farmer\n",
            "welfare through farmer exchange values amidst fluctuation shock of food needed\n",
            "prices and inflation level in the east java province. The research method of\n",
            "this paper employs the impulse response function of the Bayesian Vector\n",
            "Autoregressive (BVAR) model by using time series secondary data from May 2017\n",
            "until December 2023. This paper finds that the shock that happens to aggregate\n",
            "food prices can increase farmer exchange values even though the shock to the\n",
            "inflation level has reduced farmer exchange values and increased aggregate food\n",
            "prices.\n",
            "\n",
            "This paper provides a valuable initial analysis of how price shocks affect farmer welfare in East Java, utilizing a robust BVAR methodology. Building upon these findings, future research could delve deeper and broaden the scope in several directions:\n",
            "\n",
            "Here are some potential future work avenues:\n",
            "\n",
            "1.  **Disaggregation and Specificity of Shocks:**\n",
            "    *   **Commodity-Specific Analysis:** Instead of aggregate food prices, analyze the impact of price fluctuations for specific key commodities (e.g., rice, corn, palm oil, specific vegetables) that are dominant in East Java. The impact of a rice price shock might differ significantly from a vegetable price shock.\n",
            "    *   **Disaggregated Inflation Components:** Investigate which components of inflation (e.g., food, energy, housing) have the most significant negative impact on farmer exchange values. This could reveal more targeted policy intervention points.\n",
            "\n",
            "2.  **Inclusion of Input Costs and Other Variables:**\n",
            "    *   **Farmer Input Prices:** The current study focuses on output prices and general inflation. Future work should explicitly incorporate the prices of key agricultural inputs (fertilizers, seeds, pesticides, fuel, labor wages) into the model. An increase in food prices might increase FEV, but if input costs rise even faster, the net effect on well-being could still be negative.\n",
            "    *   **Climate and Weather Variables:** Agriculture is highly susceptible to weather shocks. Including variables for rainfall, temperature anomalies, or extreme weather events could provide a more comprehensive understanding of factors influencing both prices and farmer welfare.\n",
            "    *   **Government Policies and Subsidies:** Analyze the buffering effect or exacerbating role of government policies like price stabilization, input subsidies, or credit programs on farmer welfare amidst price and inflation shocks.\n",
            "\n",
            "3.  **Exploring Causal Mechanisms and Heterogeneity:**\n",
            "    *   **Understanding the \"Positive Food Price Shock\" Effect:** The finding that aggregate food price shocks *increase* FEV is intriguing. Future research could explore the underlying mechanisms more deeply. Is it due to market power dynamics, farmers being net sellers of food, or slow transmission of food price increases to other consumer goods relevant to farmers' expenditures?\n",
            "    *   **Farmer Heterogeneity:** Not all farmers are the same. Future work could investigate if the impact of price and inflation shocks varies based on:\n",
            "        *   Farm size (smallholder vs. large-scale)\n",
            "        *   Type of farming (subsistence vs. commercial)\n",
            "        *   Crop diversity\n",
            "        *   Access to markets, credit, and information\n",
            "        *   Geographic location within East Java (e.g., coastal vs. mountainous regions)\n",
            "        This would likely require micro-level or panel data, moving beyond aggregated time series.\n",
            "\n",
            "4.  **Methodological Extensions:**\n",
            "    *   **Structural VAR (SVAR) Models:** To better identify and distinguish between demand-side and supply-side shocks to food prices, and to impose more theoretical restrictions on the relationships.\n",
            "    *   **Non-linear Models:** Investigate if the responses are asymmetric or non-linear. For example, a large price increase might have a different impact on FEV than a small one, or farmers might react differently to price increases vs. decreases. (e.g., Threshold VAR, Regime-Switching VAR).\n",
            "    *   **Forecasting and Policy Simulation:** Use the enhanced model to forecast future FEV and food prices, and to simulate the potential impact of various policy interventions (e.g., different levels of subsidies, price floors/ceilings) on farmer welfare.\n",
            "\n",
            "5.  **Broader Measures of Farmer Well-being:**\n",
            "    *   **Multi-dimensional Well-being:** While FEV is a good economic indicator, \"well-being\" is multi-faceted. Future work could integrate other indicators such as food security levels, health outcomes, access to education, or even subjective well-being measures through household surveys.\n",
            "    *   **Link to Poverty and Income Inequality:** Analyze how price uncertainty and inflation disproportionately affect different income strata among farmers and its implications for rural poverty and inequality in East Java.\n",
            "\n",
            "By pursuing these avenues, future research can build a more nuanced, comprehensive, and policy-relevant understanding of the challenges faced by farmers in East Java and contribute to more effective strategies for enhancing their well-being.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oDF9q40_b1Yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Atividade: RAG API\n"
      ],
      "metadata": {
        "id": "1Z9R9hLCb2CB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Atividade: RAG API\n",
        "\n",
        "Seu desafio é construir um pipeline de Geração Aumentada por Recuperação (RAG) similar ao demonstrado no notebook, utilizando uma API de LLM de sua escolha (como a API do Google Gemini, OpenAI, ou Anthropic) e uma Base de Conhecimento  externa. O objetivo é aprimorar as respostas do LLM ao fornecer contexto relevante, mitigando as limitações do modelo pré-treinado, como alucinações e informações desatualizadas. Esta base de conhecimento deve ser uma API aberta (sem autenticacao) a sua escolha.\n",
        "Você deve então alimentar um Banco de Dados Vetorial (por exemplo, usando ChromaDB ) para armazenar os embeddings  de seus documentos. Em seguida, implemente a lógica de recuperação (Retrieval), onde uma consulta de usuário é convertida em um vetor e usada para buscar documentos semanticamente semelhantes  no seu banco de dados. Finalmente, crie um prompt para o LLM que combine a consulta original com os documentos recuperados, garantindo que o modelo gere uma resposta factual e fundamentada  apenas no contexto que você forneceu. Você precisará documentar a API e os modelos de embeddings escolhidos, detalhado as etapas do pipeline RAG\n"
      ],
      "metadata": {
        "id": "Y_C4d6N-b2Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD\n",
        "from langchain_core.documents import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "import os\n",
        "API_KEY = 'yourkey'\n",
        "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "ns={'atom': 'http://www.w3.org/2005/Atom'}\n",
        "\n",
        "embed_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def doc_from_xml(entry,a_section, a_theme):\n",
        "  title_element = entry.find('atom:title', ns)\n",
        "  title = title_element.text.strip() if title_element is not None and title_element.text is not None else \"No Title\"\n",
        "\n",
        "  published_element = entry.find('atom:published', ns)\n",
        "  published = published_element.text.strip() if published_element is not None and published_element.text is not None else \"No Published Date\"\n",
        "\n",
        "  id_element = entry.find('atom:id', ns)\n",
        "  entry_id = id_element.text.strip() if id_element is not None and id_element.text is not None else \"No Entry ID\"\n",
        "\n",
        "  summary_element = entry.find('atom:summary', ns)\n",
        "  summary = summary_element.text.strip() if summary_element is not None and summary_element.text is not None else \"No Summary\"\n",
        "\n",
        "  authors = []\n",
        "  for author_element in entry.findall('atom:author', ns):\n",
        "      name_element = author_element.find('atom:name', ns)\n",
        "      if name_element is not None and name_element.text is not None:\n",
        "          authors.append(name_element.text.strip())\n",
        "  author_names = \", \".join(authors) if authors else \"No Authors\"\n",
        "  doc = Document(\n",
        "    page_content=summary,\n",
        "    metadata={\n",
        "        \"section\": a_section,\n",
        "        \"theme\": a_theme,\n",
        "        \"title\": title,\n",
        "        \"published\": published,\n",
        "        \"author_names\": author_names,\n",
        "        \"entry_id\": entry_id})\n",
        "  return doc\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "xEx = \"\"\"<?xml version='1.0' encoding='UTF-8'?>\n",
        "<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n",
        "  <id>https://arxiv.org/api/9VsrqgLtBqizNdn2qfxBueBGmP4</id>\n",
        "  <title>arXiv Query: search_query=cat:cs.AI AND abs:java&amp;id_list=&amp;start=0&amp;max_results=1</title>\n",
        "  <updated>2025-12-08T21:40:34Z</updated>\n",
        "  <link href=\"https://arxiv.org/api/query?search_query=cat:cs.AI+AND+abs:java&amp;start=0&amp;max_results=1&amp;id_list=\" type=\"application/atom+xml\"/>\n",
        "  <opensearch:itemsPerPage>1</opensearch:itemsPerPage>\n",
        "  <opensearch:totalResults>273</opensearch:totalResults>\n",
        "  <opensearch:startIndex>0</opensearch:startIndex>\n",
        "  <entry>\n",
        "    <id>http://arxiv.org/abs/2407.03941v2</id>\n",
        "    <title>Narrow Transformer: StarCoder-Based Java-LM For Desktop</title>\n",
        "    <updated>2024-09-07T11:40:47Z</updated>\n",
        "    <link href=\"https://arxiv.org/abs/2407.03941v2\" rel=\"alternate\" type=\"text/html\"/>\n",
        "    <link href=\"https://arxiv.org/pdf/2407.03941v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
        "    <summary>This paper presents NT-Java-1.1B, an open-source specialized code language model built on StarCoderBase-1.1B, designed for coding tasks in Java programming. NT-Java-1.1B achieves state-of-the-art performance, surpassing its base model and majority of other models of similar size on MultiPL-E Java code benchmark. While there have been studies on extending large, generic pre-trained models to improve proficiency in specific programming languages like Python, similar investigations on small code models for other programming languages are lacking. Large code models require specialized hardware like GPUs for inference, highlighting the need for research into building small code models that can be deployed on developer desktops. This paper addresses this research gap by focusing on the development of a small Java code model, NT-Java-1.1B, and its quantized versions, which performs comparably to open models around 1.1B on MultiPL-E Java code benchmarks, making them ideal for desktop deployment. This paper establishes the foundation for specialized models across languages and sizes for a family of NT Models.</summary>\n",
        "    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
        "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
        "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
        "    <published>2024-07-04T13:54:24Z</published>\n",
        "    <arxiv:comment>Updated Authors list</arxiv:comment>\n",
        "    <arxiv:primary_category term=\"cs.SE\"/>\n",
        "    <author>\n",
        "      <name>Kamalkumar Rathinasamy</name>\n",
        "    </author>\n",
        "  </entry>\n",
        "</feed>\n",
        "\"\"\"\n",
        "print(doc_from_xml(ET.fromstring(xEx),'s1','t1'))\n",
        "\n",
        "def store_documents_from_xml(root, section, theme, vectordb):\n",
        "  documents_from_xml = []\n",
        "  for entry in root.findall('atom:entry', ns):\n",
        "    doc = doc_from_xml(entry,section, theme)\n",
        "    documents_from_xml.append(doc)\n",
        "    print('- ',doc.metadata['title'])\n",
        "  vectordb.add_documents(documents_from_xml)\n",
        "  return documents_from_xml\n",
        "\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "MAIN_THEME = 'java'\n",
        "MAIN_SECTION = 'econ.GN'\n",
        "sections = ['econ.GN','cs.AI']\n",
        "qty=100\n",
        "vectorstore = Chroma(embedding_function=embed_model,persist_directory='./vectordb')\n",
        "print(\"Empty ChromaDB collection created successfully!\")\n",
        "\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "for sec in sections:\n",
        "  print(f\"Processing section: {sec}\")\n",
        "  api_url = f'https://export.arxiv.org/api/query?search_query=cat:{sec}+AND+abs:{MAIN_THEME}&start=0&max_results={qty}'\n",
        "  response = requests.get(api_url)\n",
        "  xml_data = response.text\n",
        "  root = ET.fromstring(xml_data)\n",
        "  print(len(root.findall('atom:entry', ns)))\n",
        "  docs = store_documents_from_xml(root, sec, MAIN_THEME, vectorstore)\n",
        "\n",
        "print(docs[-1])\n"
      ],
      "metadata": {
        "id": "0n_799hHwYk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN\n",
        "\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "import os\n",
        "API_KEY = 'yourkey'\n",
        "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
        "ns={'atom': 'http://www.w3.org/2005/Atom'}\n",
        "\n",
        "embed_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "MAIN_THEME = 'java'\n",
        "MAIN_SECTION = 'econ.GN'\n",
        "sections = ['econ.GN','cs.AI']\n",
        "qty=100\n",
        "vectorstore = Chroma(embedding_function=embed_model,persist_directory='./vectordb')\n",
        "\n",
        "\n",
        "def query_vector_db(db,q,k,sec,th):\n",
        "  filter_criteria = 'None'\n",
        "  if sec:\n",
        "    filter_criteria = {\"$and\": [{\"section\": sec},{\"theme\": th}]}\n",
        "    results = db.similarity_search_with_score(q, k=k, filter=filter_criteria)\n",
        "  else:\n",
        "    results = db.similarity_search_with_score(q, k=k)\n",
        "  print(f\"Filtered hybrid search results for query: '{q}' (k={k}) with filter {filter_criteria}\")\n",
        "  if results:\n",
        "      for doc, score in results:\n",
        "          print(\"-\" * 50)\n",
        "          print(f'Score: {score}')\n",
        "          print(f\"Document Content: {doc.page_content}\")\n",
        "          print(f\"Document Metadata: {doc.metadata}\")\n",
        "          return doc\n",
        "  else:\n",
        "      print(\"No documents found matching the filter and query.\")\n",
        "\n",
        "\n",
        " #query = \"agricultural/farming province\"\n",
        "#econ.GN\n",
        "qq = input('Enter your query: ')\n",
        "ss = input('Enter your section: ')\n",
        "tt = None\n",
        "if ss is not None and ss != '':\n",
        "  tt = input('Enter your theme: ')\n",
        "query_vector_db(vectorstore,qq,5,ss,tt)\n",
        "\n",
        "#query = \"agricultural/farming province\"\n",
        "#econ.GN\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import os\n",
        "GCP_MODEL = 'gemini-2.5-flash'\n",
        "GOOGLE_API_KEY = 'yourkey'\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "qq = input('Enter your Topic: ')\n",
        "ss = input('Enter your Section: ')\n",
        "tt = None\n",
        "if ss is not None and ss != '':\n",
        "  tt = input('Enter your Theme: ')\n",
        "resp = query_vector_db(vectorstore,qq,1,ss,tt)\n",
        "\n",
        "PROMPT='''\n",
        "Please answer the question {q} regarding the paper abstract below:\n",
        "{abs}\n",
        "'''\n",
        "question = input('Enter your question/prompt: ')\n",
        "p =PROMPT.format(q=question,abs=resp.page_content)\n",
        "print(p)\n",
        "client = genai.Client()\n",
        "config = types.GenerateContentConfig(\n",
        "    system_instruction='You are scientific research assistant',\n",
        "    temperature=0.9)\n",
        "chat = client.chats.create(model=GCP_MODEL,config=config)\n",
        "response = chat.send_message(question)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "7_8YwYW4wriR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w3sOXwk6wZEj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}